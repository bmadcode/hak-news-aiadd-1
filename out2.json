
> hak-news@0.1.0 summarize
> curl -X POST http://localhost:3000/api/v1/hacker-news/summarized-stories -H 'Content-Type: application/json' -d '{"numStories": 10, "numCommentsPerStory": 20, "maxSummaryLength": 300}'

{"stories":[{"by":"dayanruben","descendants":93,"id":42899703,"kids":[42901158,42901726,42901749,42899950,42900086,42899856,42901722,42902103,42900119,42900889,42901986,42900502,42900114],"score":252,"time":1738428293,"title":"Apple is open sourcing Swift Build","type":"story","url":"https://www.swift.org/blog/the-next-chapter-in-swift-build-technologies/","articleSummary":{"summary":"Apple has open-sourced **Swift Build**, a powerful and extensible build engine designed to enhance the development experience for Swift projects across multiple platforms. Swift Build, already used by Xcode to support millions of apps and Apple’s internal systems, now extends its capabilities to Linux and Windows. The build system transforms user inputs like source code into output artifacts such as libraries, command-line tools, and applications, playing a critical role in developer productivity through performance, reliability, and advanced build configurations.\n\nBuilt on the **llbuild** project, Swift Build integrates robustly with the Swift compiler, supports diverse product types, and optimizes build graphs for parallelism. By open-sourcing this engine, Apple aims to unify the build experience across platforms, addressing inconsistencies between Xcode and Swift Package Manager (SwiftPM). This move will enable a consistent cross-platform experience, improve performance, and lay the groundwork for future tooling enhancements.\n\nApple has initiated the integration of Swift Build into SwiftPM as an alternate build engine, with plans to collaborate with the community to unify build system integrations. This effort aims to provide developers with a consistent and polished experience, regardless of their IDE or target platform. The **swift-build** repository is now available on GitHub, inviting contributions and feedback from the community. This marks a significant step in advancing Swift’s build system and fostering a healthy package ecosystem.","summaryGeneratedAt":"2025-02-01T21:23:29.234Z","tokenCount":1063},"commentsSummary":{"summary":"The discussion revolves around the perception and future of the Swift programming language, primarily developed by Apple. Participants express concerns about Swift's strong ties to Apple's ecosystem, which they believe limits its broader adoption and utility compared to more universally supported languages like Python, Kotlin, or Rust. Critics argue that Swift's integration with Apple's platforms and tools, such as Xcode, makes it less appealing for developers outside the Apple ecosystem, despite its open-source nature.\n\nSome contributors compare Swift's situation to that of C# and Microsoft's .NET, noting similarities in how both languages are closely associated with their respective corporate ecosystems. They suggest that for Swift to gain wider acceptance, Apple might need to open-source more of its development tools and make Swift more platform-agnostic.\n\nThere is also a sentiment that Apple's control over Swift and its development tools could be stifling the language's potential. While some appreciate Swift's design and features, they feel that Apple's restrictive approach could hinder its growth and adoption. The conversation also touches on the broader implications of corporate control over programming languages and the importance of community-driven development in fostering innovation and adoption.\n\nOverall, the discussion highlights a tension between Swift's technical merits and its perceived limitations due to its close association with Apple, with some advocating for a more open and inclusive approach to its development and ecosystem.","summaryGeneratedAt":"2025-02-01T21:23:19.721Z","tokenCount":2041}},{"by":"todsacerdoti","descendants":5,"id":42901616,"kids":[42902205,42902214,42902168],"score":35,"time":1738439857,"title":"Python 3, Pygame, and Debian Bookworm on the Miyoo A30","type":"story","url":"https://www.jtolio.com/2025/02/py3-pygame-miyoo-a30/","articleSummary":{"summary":"The Miyoo A30, a $30-$40 handheld device from AliExpress, offers a full Linux computer with WiFi and a GPU in a compact form. Despite its limited popularity in the retro gaming community, it features a 4-core Cortex A7 CPU, Mali-400 MP GPU, 512MB RAM, and a 640x480 IPS screen, capable of emulating Nintendo 64 games. The author purchased two devices to create networked multiplayer games with their son, leveraging Python 3 and Pygame.\n\nTo set up the Miyoo A30, the author replaced the default software with spruceOS, a custom Linux image optimized for the device. However, spruceOS uses an older Linux 3.x kernel, which lacks some syscalls required by Debian Bookworm. Despite this, Python, venv, and pip still function. The author installed Debian Bookworm in a chroot environment using debootstrap, ensuring Python 3, Pygame, and other necessary packages were included.\n\nTo enable Pygame functionality, the author copied GPU and SDL drivers from spruceOS into the Debian chroot, ensuring proper graphics and input support. A custom script, `run-inside.sh`, was created to safely enter the chroot environment, and a sample Pygame app was tested. The app was made accessible in spruceOS by placing a script in the Roms/PORTS directory.\n\nWhile the setup works well, with GPU acceleration and sound functioning, the screen is rotated 90 degrees. The author temporarily addressed this by rotating the screen in Pygame. The post includes downloadable files for the Debian Bookworm image, the chroot tool, a sample game, and a script for running apps in spruceOS.","summaryGeneratedAt":"2025-02-01T21:23:29.325Z","tokenCount":2044},"commentsSummary":{"summary":"The comments discuss the potential and challenges of budget handheld devices, particularly those running Linux on ARM-based single-board computers (SBCs). **magic_hamster** praises the exploration of GPU access via Python and expresses excitement for future game development on these devices. **daghamm** notes that the CPU performance of these handhelds is comparable to the Raspberry Pi 2 and Zero W2, which can run the latest Debian, but questions why the official OS for these devices relies on outdated kernels. \n\n**wronglebowski** highlights the abundance of Linux-based handhelds on platforms like Aliexpress, emphasizing their computational potential but criticizing the poor software support. They express skepticism about improvements, citing a lack of incentive to modernize the software for these ARM SBCs. **doctorpangloss** counters by pointing out that more powerful x86 machines are often discarded and not preserved, questioning the significance of these handhelds. Finally, **nine_k** argues that x86 devices are not comparable due to their lack of portability and inferior power efficiency, making ARM-based handhelds more suitable for specific use cases. Overall, the discussion reflects a mix of optimism about the potential of budget handhelds and frustration with their software limitations.","summaryGeneratedAt":"2025-02-01T21:23:18.308Z","tokenCount":519}},{"by":"fhinson","descendants":24,"id":42901182,"kids":[42902417,42902354,42901672,42902162,42902250,42902346,42901574,42901883,42901636,42902020,42901589],"score":40,"time":1738437072,"title":"YouTube audio quality – How good does it get? (2022)","type":"story","url":"https://www.audiomisc.co.uk/YouTube/SpotTheDifference.html","articleSummary":{"summary":"The article explores the audio quality of YouTube videos, focusing on the impact of different audio codecs and sample rates. The author investigates whether the 'opus' codec provides better audio quality compared to 'aac/mp4' by analyzing audio samples from the Ralph Vaughan-Williams Society (RVWSoc), which uploads excerpts of their CD recordings to YouTube. The study compares the original uploaded audio with YouTube's processed versions, focusing on higher sample rates (48k and 44.1k) to avoid degradation from sample rate conversions.\n\nKey findings include:\n1. YouTube offers multiple audio formats with varying sample rates and bitrates, some as low as 22.05kHz, which limits frequency range.\n2. The 'opus' codec maintains the original sample rate but reduces the bitrate, while 'aac' alters both the bitrate and sample rate.\n3. Time alignment and cross-correlation techniques reveal slight delays in YouTube's processed audio, with differences of around 6.5 milliseconds.\n4. Subtracting the original audio from the YouTube-processed version shows a residual error, indicating a 20dB drop in quality, equivalent to around 10% distortion.\n5. Crest factor analysis suggests that YouTube's processing slightly alters the waveform dynamics, though the overall impact on audio quality is relatively minor.\n\nIn conclusion, while YouTube's audio processing introduces some degradation, the 'opus' codec performs comparably to 'aac' when maintaining the original sample rate. However, users may face challenges in selecting the least-altered audio version due to YouTube's opaque processing pipeline.","summaryGeneratedAt":"2025-02-01T21:23:36.610Z","tokenCount":2770},"commentsSummary":{"summary":"The discussion revolves around audio quality on platforms like YouTube and SoundCloud, with a focus on tools, codecs, and user experiences. _DeadFred_ mentions Streamliner, a tool for optimizing audio uploads across platforms, and notes differences in audio quality between YouTube and SoundCloud, attributing it to mastering standards like YouTube's -14 LUFS requirement. He also questions whether streaming services' codecs influence the need for closed captioning and if audio professionals use tools like Streamliner for decision-making.\n\ntogetheragainor highlights YouTube Music's settings for higher audio quality, offering up to 256kbps AAC and OPUS, though it's unclear if this is a premium-only feature. _DeadFred_ further wonders if tagging a video as \"music\" on YouTube improves its audio quality on YouTube Music.\n\nThe conversation shifts to technical aspects, with lazka referencing a Hydrogenaudio FAQ on codec comparisons, noting that OPUS operates at 48kHz. xipix criticizes YouTube's audio time-stretching algorithm, particularly for music practice, where playback at varying speeds is essential. ajsnigrutin counters that music is typically played at 1x speed, but xipix emphasizes the importance of adjustable speeds for learning instruments.\n\njurmous mentions YouTube Premium's experimental high-quality audio feature, available until February 22, 2023, for iOS and Android users. londons_explore speculates that YouTube prioritizes loading times over audio quality, as viewers may not notice subtle audio differences. jorvi and echoangle discuss adaptive bitrate streaming, suggesting low-bitrate starts followed by higher-quality streams to balance loading times and audio quality. mungoman2 and kccqzy elaborate on encoding strategies, with kccqzy noting that YouTube already uses chunked streaming (m3u8 files), making such encoding tricks unnecessary.\n\nsupertrope points out that YouTube's primary audio codecs, Opus and AAC, use variable bitrate (VBR) by default. deathanatos critiques YouTube's page load performance, suggesting that reducing page bloat could improve loading times more effectively than lowering audio bitrates. londons_explore questions how much data is loaded during video-to-video navigation, assuming most JavaScript is cached.\n\nbaal80spam expresses dissatisfaction with YouTube's audio quality, while nexus7556 and jeffbee note the rapid evolution of YouTube's backend, making it difficult to assess current practices. Overall, the discussion highlights the complexities of audio quality on streaming platforms, user preferences, and technical challenges in balancing quality and performance.","summaryGeneratedAt":"2025-02-01T21:23:24.526Z","tokenCount":2173}},{"by":"gnufx","descendants":2,"id":42893196,"kids":[42901741],"score":28,"time":1738363350,"title":"Discovery of collagen in fossil bone could unlock new insights into dinosaurs","type":"story","url":"https://news.liverpool.ac.uk/2025/01/31/discovery-of-collagen-in-fossil-bone-could-unlock-new-insights-into-dinosaurs/","articleSummary":{"summary":"Discovery of collagen in fossil bone could unlock new insights into dinosaurs from https://news.liverpool.ac.uk/2025/01/31/discovery-of-collagen-in-fossil-bone-could-unlock-new-insights-into-dinosaurs/ unretrievable, try yourself by visiting the link.","summaryGeneratedAt":"2025-02-01T21:23:18.936Z","tokenCount":0},"commentsSummary":{"summary":"The discussion revolves around the discovery of what was initially believed to be soft tissues and blood vessels in T-rex bones over 15 years ago. This finding, led by researcher Mary Schweitzer, was groundbreaking as it suggested the possibility of preserved organic material from dinosaurs. However, subsequent research by Thomas G. Kaye, Gary Gaugler, and Zbigniew Sawlowicz challenged this interpretation. They argued that the structures identified as soft tissue were actually framboids—microscopic mineral structures that can resemble biological tissues. Additionally, carbon dating indicated that the material was modern, not prehistoric, leading them to conclude that the supposed soft tissue was essentially contamination, likened to \"pond scum.\" This reinterpretation has led to skepticism about the original findings, though the debate continues in the scientific community. The link provided directs to a study published in PLOS ONE, which details the alternative explanation for the observed structures.","summaryGeneratedAt":"2025-02-01T21:23:17.893Z","tokenCount":449}},{"by":"tosh","descendants":80,"id":42899713,"kids":[42900094,42900071,42900442,42901649,42901256,42900455,42900088,42900229],"score":132,"time":1738428361,"title":"Bzip3: A spiritual successor to BZip2","type":"story","url":"https://github.com/kspalaiologos/bzip3","articleSummary":{"summary":"BZip3 is an advanced compression tool designed as a successor to BZip2, offering higher compression ratios and better performance. It utilizes an order-0 context mixing entropy coder, a fast Burrows-Wheeler transform with suffix arrays, and a combination of RLE, LZ77-style string matching, and PPM-style context modeling. BZip3 is particularly effective for compressing text and code. Installation can be done via source compilation or package managers like Homebrew on macOS.\n\nPerformance benchmarks were conducted using Perl source code, comparing BZip3 with other compressors like xz, bzip2, and Zstandard. BZip3 demonstrated superior compression ratios and faster decompression times, especially in parallel processing scenarios. Additional tests with lrzip for long-range deduplication further highlighted BZip3's efficiency.\n\nThe tool is built with careful attention to reliability and performance, though users are cautioned about the inherent risks of data loss due to the complexity of the algorithms. BZip3's performance varies by compiler and architecture, with optimal results on x64 Linux using clang13. It supports multiple architectures including x86, ARM, and more.\n\nLicensing for BZip3 is under LGPLv3, with specific components like the Burrows-Wheeler transform and LZP code licensed under Apache 2.0. The project acknowledges contributions from Ilya Grebnov for the libsais library and Caleb Maclennan for configuring autotools. BZip3 represents a significant step forward in compression technology, balancing speed, efficiency, and reliability.","summaryGeneratedAt":"2025-02-01T21:23:32.634Z","tokenCount":1973},"commentsSummary":{"summary":"The discussion revolves around the Burrows-Wheeler Transform (BWT), its intuitive understanding, and its application in data compression. Users express admiration for BWT's algorithmic elegance, though some find its underlying intuition elusive. BWT is noted for its uniqueness, lacking non-trivial variations, and is often combined with other compression techniques like Huffman or arithmetic coding in tools like Bzip2 and Bzip3.\n\nA key insight is that BWT improves compressibility by clustering similar contexts, making it easier for subsequent compression algorithms to exploit predictability. Unlike Huffman coding, which encodes symbols independently, BWT implicitly models the data by sorting symbols based on their contexts, allowing for more efficient compression with longer context lengths without the computational overhead of adaptive models.\n\nUsers also discuss the practical performance of BWT-based compression tools like Bzip3, comparing them to other algorithms like Zstandard (zstd). Benchmarks show that Bzip3, which uses BWT combined with arithmetic coding, performs competitively, especially on textual data, though its effectiveness varies depending on the data type and block sizes. The discussion highlights that data compression is an art, balancing compression ratio, speed, and memory usage, with BWT-based methods offering a unique trade-off suited for modern hardware.","summaryGeneratedAt":"2025-02-01T21:23:20.280Z","tokenCount":4069}},{"by":"phiresky","descendants":56,"id":42897120,"kids":[42902471,42898709,42897723,42897707,42897763,42898108,42899589,42897462,42900045,42900755,42897567,42897559,42900427,42897414,42898330,42899700,42897920,42899785,42897749,42898518,42898456,42898549,42897458,42897552,42900621,42898312,42898338,42897727,42898454,42897565,42898760,42899466,42897485,42897515,42899935,42897460,42899443],"score":290,"time":1738402026,"title":"Visualizing all books of the world in ISBN-Space","type":"story","url":"https://phiresky.github.io/blog/2025/visualizing-all-books-in-isbn-space/","articleSummary":{"summary":"The article discusses a project aimed at visualizing all books in the world using their International Standard Book Numbers (ISBNs) in a two-dimensional space. ISBNs are 13-digit numbers assigned to published books, with the first three digits fixed and the last being a checksum, leaving two billion possible unique identifiers. The project, developed by Anna’s Archive, a shadow library, seeks to create an interactive visualization of this \"ISBN-Space.\"\n\nThe visualization uses a custom space-filling curve called the \"Bookshelf-Curve,\" which organizes ISBNs in a way that resembles a bookshelf, making it easier to explore and understand the data. The project involves generating map tiles for different ISBN prefixes, storing data like publication years and book availability, and rendering these using GLSL fragment shaders for flexibility in color schemes and data transformations.\n\nThe visualization allows users to filter by publication year, search for individual books, and create custom visualizations. At maximum zoom, each pixel represents a single book, styled to look like a physical book on a shelf. The project also includes features like barcodes for each book and color-coded publisher ranges.\n\nThe backend relies on static file hosting, while the frontend uses ThreeJS, React, and MobX for creating interactive, GPU-accelerated 2D/3D scenes. The project highlights the challenges and solutions in visualizing large datasets, such as performance optimization and smooth navigation through the ISBN-Space. The source code is available on GitHub, offering a flexible tool for exploring the vast collection of published books.","summaryGeneratedAt":"2025-02-01T21:23:31.394Z","tokenCount":4148},"commentsSummary":{"summary":"The discussion revolves around a visualization of ISBNs (International Standard Book Numbers) presented on Anna's Archive, a digital library project. The visualization maps out books in a grid-like format, allowing users to explore titles, publishers, and other metadata. However, several users point out limitations and inaccuracies in the representation. For instance, ISBNs can be assigned multiple times to different books, and some books may have invalid ISBNs, making the claim of \"all books in ISBN space\" an overstatement. Additionally, the visualization reflects Anna's Archive's collection bias, with certain languages and regions underrepresented.\n\nUsers also discuss the practicalities of ISBN assignment, noting that publishers buy blocks of ISBNs and assign them arbitrarily, which can lead to inconsistencies in the visualization. Some books available online or in bookstores are missing from the archive, possibly due to gaps in data sources like WorldCat or Google Books. The visualization itself is praised for its creativity and detail, with features like zooming in to see book covers and metadata. However, suggestions for improvement include decluttering the interface, adding sorting options, and better distinguishing between different editions (e.g., paperback, hardcover, e-books).\n\nThe conversation also touches on the history and structure of ISBNs, including the \"Bookland\" prefix (978) and its expansion to 979, originally designated for music. The idea of extraterrestrial publishing is humorously considered, with speculation about future prefixes for books published on the Moon or Mars. Overall, the visualization is celebrated as a fascinating and ambitious project, though it highlights the complexities and challenges of organizing and representing the vast world of published works.","summaryGeneratedAt":"2025-02-01T21:23:21.171Z","tokenCount":2458}},{"by":"adam_carrigan","id":42902267,"score":1,"time":1738443634,"title":"MindsDB (YC W20) Is Hiring an Office Manager in SF","type":"job","url":"https://grnh.se/83c3fffa7us","articleSummary":{"summary":"MindsDB, a San Francisco-based AI startup founded in 2017, is seeking an Office Manager to oversee its dynamic workspace. The company, recognized by Forbes and Gartner for its innovative AI-Data platform, has grown significantly with over $55M in funding and a global community of contributors. The Office Manager will play a crucial role in maintaining a safe, efficient, and vibrant office environment, coordinating events, and fostering a welcoming culture. Responsibilities include facilities management, vendor relations, security, inventory management, and event logistics. The ideal candidate will have 3+ years of office management experience, strong organizational and communication skills, and the ability to thrive in a fast-paced startup environment. Benefits include flexible working hours, competitive compensation, health insurance, 401k matching, unlimited PTO, and various budgets for remote setup, learning, wellbeing, and commuter expenses. MindsDB is committed to diversity, equality, and inclusion, offering equal opportunities to all applicants. The salary range for this role is $75,000 - $95,000 USD.","summaryGeneratedAt":"2025-02-01T21:23:18.198Z","tokenCount":1554},"commentsSummary":{"summary":"No comments available for this story.","summaryGeneratedAt":"2025-02-01T21:23:09.940Z","tokenCount":0}},{"by":"davikr","descendants":133,"id":42877910,"kids":[42902017,42902474,42898868,42902482,42902452,42899160,42878917,42854789,42899188,42898776,42902453,42880817,42901777,42858807,42902324,42898839,42853350,42851039,42902299,42898297,42899314,42902224,42902228,42902334,42901970,42902121,42878274,42899588,42899072,42902454,42878307],"score":110,"time":1738246880,"title":"String of recent killings linked to Bay Area rationalist 'death cult'","type":"story","url":"https://www.sfgate.com/bayarea/article/bay-area-death-cult-zizian-murders-20064333.php","articleSummary":{"summary":"Curtis Lind, an 82-year-old Vallejo, California, landlord, was stabbed to death on January 17, 2025, near his property, weeks before he was set to testify in a case involving a 2022 samurai sword attack by tenants living on his land. Lind had survived the earlier attack, which left one assailant dead and cost him an eye. Maximilian Snyder, a 22-year-old Oxford-educated data scientist, has been charged with Lind’s murder. Separately, in Burlington, Vermont, 21-year-old data scientist Teresa Youngblut was charged in connection with a shootout that killed a Border Patrol agent and a German citizen, Felix Bauckholt, after a traffic stop near the Canadian border. Authorities allege Youngblut and Bauckholt were under surveillance for suspicious behavior, including carrying firearms and wrapping items in aluminum foil. \n\nInvestigations reveal a potential link between the two incidents, with weapons tied to the Vallejo homicide and another double homicide in Pennsylvania. Both Snyder and Youngblut are connected to a fringe online group called the “Zizians,” a radical offshoot of the Rationalism movement, which has been described as a “death cult.” The group, named after a former tenant of Lind’s, has been linked to violent ideologies. Snyder is held without bail and faces capital murder charges, while further details about the group’s activities continue to emerge.","summaryGeneratedAt":"2025-02-01T21:23:34.083Z","tokenCount":1293},"commentsSummary":{"summary":"The discussion revolves around a series of events and articles related to a group known as the Zizians, associated with the Rationalism movement, and their alleged involvement in violent activities. The conversation begins with a reference to a later article by the same author, suggesting that readers should either read both articles or neither to get a complete picture. \n\nA user, t_mann, questions the connection between Rationalism, Effective Altruism, and Long-Termism, hinting at a possible overlap in ideologies and funding strategies, similar to those associated with figures like SBF (Sam Bankman-Fried). Another user, iamthepieman, shares a personal connection to the story, noting the proximity of the events to their location and expressing disappointment in how the Rationalism community has been portrayed negatively.\n\nThe discussion includes procedural questions about why the story is appearing on Hacker News (HN) now, with dang explaining that previous submissions didn't gain much traction. There's also clarification about the term \"LW,\" which stands for Less Wrong, a Rationalism forum. \n\nThe conversation delves into the specifics of the Zizian group, with references to a 2023 post on LessWrong warning of potential violence within the community. Jessica Taylor, a friend of one of the individuals involved, describes the group as a \"death cult.\" The NY Post's framing of the group as a \"radical vegan trans cult\" is critiqued, with users arguing that the trans element is overstated and that the real story is more complex and tied to the broader Rationalism movement.\n\nTrasmatta and romaaeterna debate the accuracy of the NY Post's reporting, with Trasmatta pointing out the publication's agenda to portray transgender people negatively. Romaaeterna seeks clarification on whether the trans element is a significant part of the cult, to which Trasmatta responds that it is not a major factor and that the story is more about the Rationalism movement.\n\nIn summary, the discussion highlights the complexities and controversies surrounding the Zizian group, the Rationalism movement, and the media's portrayal of these events, with particular attention to the misrepresentation of transgender individuals in the narrative.","summaryGeneratedAt":"2025-02-01T21:23:23.010Z","tokenCount":2426}},{"by":"heraclius1729","descendants":4,"id":42894660,"kids":[42901705,42902140,42901829,42901687],"score":21,"time":1738373024,"title":"Using eqn for static website equation generation","type":"story","url":"https://douglasrumbaugh.com/post/eqn-mathml/","articleSummary":{"summary":"The author developed a static site generator to replace Hugo for their personal website, aiming to eliminate JavaScript, including MathJax for rendering equations. Initially, they considered using SVGs to render equations but encountered issues with inconsistent sizing, accessibility, and inline formatting. They then discovered that `eqn`, a tool they were using, could generate MathML, an XML-based language for mathematical expressions, which resolved these issues. MathML is well-supported in modern browsers and allows for semantic markup and text selection, though it may not work in some niche browsers.\n\nThe author implemented a two-pass system: one for block equations, allowing a choice between MathML and SVGs, and another for inline equations using MathML. They used `eqn` with the `-T MathML` option to generate MathML and cleaned up the output with `sed` to remove unnecessary groff directives. For SVGs, they used a pipeline of tools to convert groff output into SVG images.\n\nDespite its advantages, the approach has limitations, such as poor Unicode support, unescapable delimiters, and unsupported advanced `eqn` features. The author provided workarounds, like avoiding problematic characters and using HTML named entities. They concluded that while the system has some rough edges, it effectively eliminates JavaScript and simplifies equation rendering for their website.","summaryGeneratedAt":"2025-02-01T21:23:28.289Z","tokenCount":3224},"commentsSummary":{"summary":"The discussion revolves around methods to render mathematical equations in web pages without relying on client-side JavaScript. The author appreciates the goal of avoiding JavaScript for rendering math, acknowledging the trade-offs between using images, SVG, and MathML. One suggested approach is to use tools like MathJax or KaTeX at build time (e.g., during Hugo build) to generate static pages that do not require JavaScript for rendering. Links to MathJax and KaTeX documentation are provided for server-side rendering.\n\nAnother commenter, elashri, offers two additional options: 1) Using MathJax or KaTeX for server-side rendering, and 2) Utilizing Hugo with Pandoc as a Markdown renderer, which allows LaTeX equations in Markdown to be converted into MathML during the build process. They also mention tex4ht as an option, though it is more suited for converting entire LaTeX documents to HTML and requires manual integration with Hugo.\n\nEichin points out a bug report related to Unicode issues in a codebase, noting the complexity of resolving such problems. Lastly, cluckindan suggests that a LaTeX static site generator might have been a better fit for the requirements, implying that it could simplify the process of rendering mathematical content without JavaScript. Overall, the discussion highlights various strategies for rendering math on static websites while minimizing reliance on client-side scripting.","summaryGeneratedAt":"2025-02-01T21:23:19.054Z","tokenCount":954}},{"by":"johnneville","descendants":69,"id":42881367,"kids":[42901141,42902157,42900499,42902379,42899926,42900181,42899869,42901526,42899993,42901484,42899802,42900783],"score":184,"time":1738266033,"title":"Archivists work to save disappearing data.gov datasets","type":"story","url":"https://www.404media.co/archivists-work-to-identify-and-save-the-thousands-of-datasets-disappearing-from-data-gov/","articleSummary":{"summary":"Since Donald Trump's inauguration, over 2,000 datasets have disappeared from data.gov, the U.S. government's largest open data repository. While some data may still exist on individual agency websites or third-party platforms, many datasets, particularly from agencies like the Department of Energy, NOAA, and the EPA, are no longer accessible through data.gov. Researchers, including Harvard's Jack Cushman, are working to archive and analyze these deletions, but the process is complicated by data.gov's role as an aggregator rather than a host. Some deletions may be routine administrative changes, while others appear linked to the Trump administration's policies, such as the removal of climate change and diversity-related data. Archivists emphasize the challenges of preserving digital government information, which is more fragile than print-era documents. Efforts like the End of Term Web Archive are underway to save as much data as possible, but the task remains complex and time-intensive. The broader issue highlights the vulnerability of digital government data and the need for better preservation policies.","summaryGeneratedAt":"2025-02-01T21:23:30.135Z","tokenCount":2427},"commentsSummary":{"summary":"The discussion revolves around the archiving and potential removal of datasets from data.gov, with a focus on environmental science agencies like the EPA, NOAA, and the Department of Energy. User **cle** has been archiving data.gov for over a year and notes significant fluctuations in dataset counts, including drops of ~10,000 datasets in 2024. They suggest building a tool to analyze these changes but acknowledge the data is not currently formatted for such analysis. A related thread discusses the disappearance of CDC data, raising concerns about targeted removals.\n\nThe conversation shifts to the role of the executive branch in spending appropriated funds, with users debating whether the President can legally withhold or redirect funds. **arcbyte** and **_DeadFred_** argue that while the President has some discretion, they cannot act arbitrarily, citing the Administrative Procedure Act (APA) of 1946 and a Supreme Court ruling against Trump for similar actions. **ks2048** and **lowercased** express concerns about the erosion of norms, particularly under the Trump administration, and the potential for misuse of power, including by figures like Elon Musk, who is speculated to influence payment systems or access IRS data for personal gain.\n\nThe discussion also touches on the termination of EPA employees, with **TheBlight** and **dangrossman** debating the legality of immediate terminations under 5 CFR § 315.804. **_DeadFred_** reiterates that such actions must not be arbitrary or capricious, as per the APA.\n\nOverall, the thread highlights concerns about transparency, the rule of law, and the potential for executive overreach, particularly in the context of environmental data and federal spending. Users express skepticism about accountability and the preservation of democratic norms.","summaryGeneratedAt":"2025-02-01T21:23:22.339Z","tokenCount":2396}}],"meta":{"fetchedAt":"2025-02-01T21:23:36.610Z","processingTimeMs":26970,"storiesRetrieved":10,"totalCommentsRetrieved":10,"totalTokensUsed":37981}}